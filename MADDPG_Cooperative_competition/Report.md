[//]: # (Image References)

[image1]: media/actor_network.png "Actor Network"
[image2]: media/critic_network.png  "Critic Network"
[image3]: media/graph.png   "Reward Graph"
[image4]: media/compare_graph.png "Compare graph"
[image5]: media/compare_table.png "Compare table"

# Tennis: Collaboration and competition

## Introduction

This project is the third in [Udacity's Deep Reinforcement Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).

## Environment

In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01. Thus, the goal of each agent is to keep the ball in play.

The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.

The task is episodic, and in order to solve the environment, your agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). Specifically,

 - After each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores.
 - This yields a single score for each episode.

## Solution

This implementation uses Multi Agent Deep Deterministic Policy Gradient (MADDPG) describes in [this paper](https://arxiv.org/pdf/1706.02275.pdf)

It uses two neural networks:
 - Actor network
 - Critic Network

In order to stabilize the model, we will not update the networks used to predict the action, but intermediate networks that will be updated regularly in order to obtain efficient target networks.

This the description of actor network:

![Actor network][image1]

And the description of critic network

![Critic network][image2]

## Components

### Replay buffer

In this implementation, agents are a mix of competition and collaboration, because even if they play against each other, they share the same experience from the batch generated by the replay buffer.

### Ornstein-Uhlenbeck process

In order not to remain in an exploitation approach and to add a part of exploration, the Ornstein-Uhlenbeck process was implemented with the objective of adding some noise to the actions taken.

## Hyperparameters

| Hyperparameter | Value  | Description  |
|---|---|---|
| Buffer size  |  1 000 000 | Number of episods in the replay buffer|
|  Batch size |  128 | Number of episodes in each batch |
|  Learn every |  2 | How often the target network is updated|
| Learn number  | 3  | |
|  LR Actor |  0.001 | Learning rate for Agent network |
|  LR Critic | 0.01  | Learning rate for Critic network |
|  Gamma |  0.99 | Discount value |
|  Tau |  0.003 | Factor to update the target network |
|  Epsilon | 1  | Discovery factor |
| Epsilon decay  | 0.99 | Discount rate for discovery factor |
| Weight decay  | 0 | Weight decay in learning phase |
| Clip grad  | 0.1 | Grandient clipping |

## Benchmark


Here is a benchmark with different hyper parameter values in order to find the best compromise.

We can see that the initial execution 'init run' was not the best. By increasing the batch size we get much better performance.

We can see that other elements also influence the execution by improving the result, but the batch size has a greater impact.

The last test allows to combine several hyperparameters that had a positive influence on the first one.

| Hyperparameter | init run  | Batch size larger  | Learn every more  |  Learn every less  |  LR actor greater | LR critic greater | Weight decay greater | gradient clipping | X max
|---|---|---|---|---|---|---|---|---|---|
| Buffer size  |  1 000 000 | 1 000 000 | 1 000 000  | 1 000 000  | 1 000 000 | 1 000 000| 1 000 000 | 1 000 000 | 1 000 000
|  Batch size |  64 | 128  |  128 |  128 | 128 | 128 | 128 | 128 | 128 | 
|  Learn every |  2 |  2 | 1  |  4 | 2 | 2 | 2 | 2 | 2 | 
| Learn number  | 3  | 3  | 3  |  3 | 3 | 3 | 3 | 3 | 3 | 
|  LR Actor |  0.001 |  0.001 | 0.001  | 0.001  | 0.01 | 0.001 | 0.001| 0.001 | 0.001 | 
|  LR Critic | 0.001  |  0.001 |  0.001 | 0.001  | 0.001 | 0.01 | 0.001 | 0.001 | 0.01 | 
|  Gamma |  0.99 | 0.99  | 0.99  | 0.99  | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 
|  Tau |  0.003 | 0.003  |  0.003 | 0.003  | 0.003 | 0.003 | 0.003 | 0.003 | 0.003 | 
|  Epsilon | 1  |  1 | 1  | 1  | 1 | 1 | 1| 1 | 1 | 
| Epsilon decay  | 0.99  |  0.99 | 0.99  | 0.99  | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 
| Weight decay  |  0 |  0 | 0  |  0 | 0 | 0 | 0.01 | 0 | 0.1 |
| Clip grad  |  off |  off | off  |  off | off | off | off | 0.1 | off |

The Xmas model is the final one. It uses some hyperparameters which improve the learning phase.

This information about these runs:

![Graph][image4]

![List][image5]

Knowing that the size of the batch had an important impact, after the first recursion, I decided to keep the value of 128. It is therefore normal that not the continuation, the executions gave better results than the first one. So I didn't keep the values with a better performance than the first one, but all the values that had an improvement after the "Batch size larger" execution.

The last is not the best, however each run is diff√©rent.

The score graph looks like that:
![Score graph][image3]
 
# Future work

Rather than using DDQN for multi_agents, I want to try another one as A3C in order to benchmark these models.

The Holy Graal doen't exist, in my mind, however I'm thinking I can improve performance with combination of some parameters.

I want to try to implement the soccer problem. I think it's an interresting one, because there is competition and cooperation with it's team and the other one. I want to see if this model can work with this problem. I'm thinking there are just little modifications for that.